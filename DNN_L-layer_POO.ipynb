{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset:\n",
    "    \"\"\" create the 2 XOR datasets, X & Y, for the DNN model \"\"\"\n",
    "    \n",
    "    # create the entry dataset X\n",
    "    def create_X(self, X_size):\n",
    "        self.X = np.random.randint(2, size=(2, X_size))\n",
    "        return self.X\n",
    "    \n",
    "    # create the label dataset Y\n",
    "    def create_Y(self, X, X_size):\n",
    "        self.Y = np.sum(X, axis=0).reshape((1,X_size))\n",
    "        self.Y[self.Y != 1] = 0\n",
    "        return self.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\" the abstract class for all activation funtion classes\"\"\"\n",
    "    \n",
    "    # the basic formula of the activation function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "    # to calculate the derivative of the activation function for the backward pass\n",
    "    def derivative(self, input):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "    # to be used to finetune the initialized weight according to the activation function set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\" all the functions related to the sigmoid activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the sigmoid function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # to calculate the derivative of the sigmoid function for the backward pass\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A)\n",
    "    \n",
    "    # to be used to finetune the initialized weight if sigmoid function is set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        return np.sqrt(1 / layer_dims)\n",
    "    \n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\" all the functions related to the tanh activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the tanh function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    \n",
    "    # to calculate the derivative of the tanh function for the backward pass\n",
    "    def derivative(self, A):\n",
    "        return 1 - A**2\n",
    "    \n",
    "    # to be used to finetune the initialized weight if tanh function is set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        return np.sqrt(1 / layer_dims)\n",
    "    \n",
    "    \n",
    "class Relu(Activation):\n",
    "    \"\"\" all the functions related to the relu activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the relu function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return (Z > 0) * Z\n",
    "    \n",
    "    # to calculate the derivative of the relu function for the backward pass\n",
    "    def derivative(self, Z):\n",
    "        return (Z > 0) * 1\n",
    "    \n",
    "    # to be used to finetune the initialized weight if relu function is set for the first layer\n",
    "    def heuristic(self, layer_b4):\n",
    "        return np.sqrt(2 / layer_b4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    \"\"\" the abstract class for all the cost functions \"\"\"\n",
    "    \n",
    "    # calculate the cost function\n",
    "    def formula(self, A, Y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # calculate the derivative of the cost function (dA[L]) for the last layer\n",
    "    def derivative(self, A, Y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LossEntropy(Cost):\n",
    "    \"\"\" Use Loss Entropy to calculate the cost \"\"\"\n",
    "    \n",
    "    # calculate the Lose Entropy cost\n",
    "    def formula(self, A, Y):\n",
    "        self.m = Y.shape[1]\n",
    "#         print(self.m)\n",
    "#         print(f'Y: {Y}')\n",
    "#         print(f'A: {A}')\n",
    "        return - np.sum((Y * np.log(A) + (1-Y) * np.log(1-A)), axis=1) / self.m\n",
    "    \n",
    "    # calculate the derivative of the Lost Entropy cost\n",
    "    def derivative(self, A, Y):\n",
    "        return - ((np.divide(Y, A)) - (np.divide(1-Y, 1-A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\" the abstract class for all layer classes \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None \n",
    "    \n",
    "    # implement forward pass\n",
    "    def forward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # implement backward pass\n",
    "    def backward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class FCLayer(Layer):\n",
    "    \n",
    "    # initialize parameters\n",
    "    def __init__(self, layer_b4, layer_after, activation):\n",
    "\n",
    "        self.activation = activation\n",
    "        self.W = np.random.randn(layer_after, layer_b4) * getattr(self.activation, 'heuristic')(self, layer_b4)\n",
    "        self.b = np.zeros((layer_after, 1))\n",
    "#         print(f'initialized W: {self.W}')\n",
    "#         print(f'initialized b: {self.b}')\n",
    "    \n",
    "    # calculate forward pass: linear fn (Z = WX + b) and non-linear (A = g(Z))\n",
    "    def forward_pass(self, X):#, activation):\n",
    "        self.A_prev = X\n",
    "        self.Z = np.dot(self.W, X) + self.b\n",
    "        self.A = getattr(self.activation, 'formula')(self, self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    # calculate backward pass: \n",
    "    # dZ = dA * g'(Z))\n",
    "    # dA[l-1] = W.T * dZ\n",
    "    def backward_pass(self, dA, learning_rate):\n",
    "        self.m = dA.shape[1]\n",
    "        \n",
    "#         print(f'shape of W: {np.shape(self.W)}')\n",
    "        self.dZ = dA * getattr(self.activation, 'derivative')(self, self.A)\n",
    "        pre = np.dot(self.W.T, self.dZ)\n",
    "        \n",
    "#         print(f'shape of dZ: {np.shape(self.dZ)}')\n",
    "        self.dW = np.dot(self.dZ, self.A_prev.T) / self.m\n",
    "#         print(f'shape of dW: {np.shape(self.dW)}')\n",
    "        \n",
    "        \n",
    "        self.W -= learning_rate * self.dW\n",
    "        self.b -= learning_rate * (np.sum(self.dZ) / self.m)\n",
    "#         print(f'updated W: {self.W}')\n",
    "#         print(f'updated b: {self.b}')\n",
    "        \n",
    "        return np.dot(self.W.T, self.dZ) # dA[l-1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\" build the whole L-layer DNN \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    # combine individual layer to form the whole DNN\n",
    "    def combine(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # train the DNN model\n",
    "    def fit(self, X, Y, iteration, loss_fn, learning_rate):\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            \n",
    "            A = X\n",
    "            for layer in self.layers:\n",
    "                A = layer.forward_pass(A)\n",
    "#                 print(f'A: {A}')\n",
    "                \n",
    "            cost = getattr(loss_fn, 'formula')(A, Y)\n",
    "            if i % 1000 == 0:\n",
    "                print(f'cost de {i}: {cost}')\n",
    "#             print(f'cost de {i}: {cost}')\n",
    "            \n",
    "            dA = getattr(loss_fn, 'derivative')(A, Y)\n",
    "            \n",
    "            for layer in reversed(self.layers):\n",
    "                dA = layer.backward_pass(dA, learning_rate)\n",
    "#                 print(f'dA: {dA}')\n",
    "        return A\n",
    "\n",
    "    # predict test dataset using the trained DNN model\n",
    "    def predict(self, X_size, A, Y, loss_fn):\n",
    "        \n",
    "        self.A_train = A\n",
    "        self.Y_train = Y\n",
    "        \n",
    "        # genreate test dataset\n",
    "        self.X_test = ds.create_X(X_size)\n",
    "        self.Y_test = ds.create_Y(self.X_test, X_size)\n",
    "        \n",
    "        self.A_test = self.X_test\n",
    "        for layer in self.layers:\n",
    "            self.A_test = layer.forward_pass(self.A_test)\n",
    "        \n",
    "        cost = getattr(loss_fn, 'formula')(self.A_test, self.Y_test)\n",
    "        print(f'cost of test dataset: {cost}')\n",
    "        \n",
    "        self.A_train = (self.A_train > 0.5) * 1\n",
    "        self.accuracy_train = (self.A_train == self.Y_train) * 1\n",
    "        print(f'Accuracy of the train dataset: {np.average(self.accuracy_train) * 100}%')\n",
    "        \n",
    "        self.A_test = (self.A_test > 0.5) * 1\n",
    "        self.accuracy_test = (self.A_test == self.Y_test) * 1\n",
    "        print(f'Accuracy of the train dataset: {np.average(self.accuracy_test) * 100}%')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost de 0: [0.69310324]\n",
      "cost de 1000: [0.09215966]\n",
      "cost de 2000: [0.02284007]\n",
      "cost de 3000: [0.01184969]\n",
      "cost de 4000: [0.00782838]\n",
      "cost de 5000: [0.00572317]\n",
      "cost de 6000: [0.00449707]\n",
      "cost de 7000: [0.00368809]\n",
      "cost de 8000: [0.00312169]\n",
      "cost de 9000: [0.00270093]\n",
      "cost of test dataset: [0.00248328]\n",
      "Accuracy of the train dataset: 100.0%\n",
      "Accuracy of the train dataset: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# define variables\n",
    "X_size = 4000\n",
    "learning_rate = 0.1\n",
    "loss_fn = LossEntropy()\n",
    "iteration = 10000\n",
    "\n",
    "# generate train dataset\n",
    "ds = CreateDataset()\n",
    "X = ds.create_X(X_size)\n",
    "Y = ds.create_Y(X, X_size)\n",
    "\n",
    "# define each layer and combine them to build the whole DNN\n",
    "net = Network()\n",
    "net.combine(FCLayer(2, 3, Relu))\n",
    "net.combine(FCLayer(3, 1, Sigmoid))\n",
    "\n",
    "# train the DNN model\n",
    "A = net.fit(X, Y, iteration, loss_fn, learning_rate)\n",
    "\n",
    "# predict a result with a test dataset using the trained DNN model\n",
    "net.predict(X_size, A, Y, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
