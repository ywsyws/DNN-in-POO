{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset:\n",
    "    \"\"\" create the 2 XOR datasets, X & Y, for the DNN model \"\"\"\n",
    "    \n",
    "    # create the entry dataset X\n",
    "    def create_X(self, X_size):\n",
    "        self.X = np.random.randint(2, size=(2, X_size))\n",
    "        return self.X\n",
    "    \n",
    "    # create the label dataset Y\n",
    "    def create_Y(self, X):\n",
    "        self.Y = np.sum(X, axis=0).reshape((1,4))\n",
    "        self.Y[self.Y != 1] = 0\n",
    "        return self.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\" the abstract class for all activation funtion classes\"\"\"\n",
    "    \n",
    "    # the basic formula of the activation function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "    # to calculate the derivative of the activation function for the backward pass\n",
    "    def derivative(self, input):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "    # to be used to finetune the initialized weight according to the activation function set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\" all the functions related to the sigmoid activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the sigmoid function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # to calculate the derivative of the sigmoid function for the backward pass\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A)\n",
    "    \n",
    "    # to be used to finetune the initialized weight if sigmoid function is set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        return np.sqrt(1 / layer_dims)\n",
    "    \n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\" all the functions related to the tanh activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the tanh function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    \n",
    "    # to calculate the derivative of the tanh function for the backward pass\n",
    "    def derivative(self, A):\n",
    "        return 1 - A**2\n",
    "    \n",
    "    # to be used to finetune the initialized weight if tanh function is set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        return np.sqrt(1 / layer_dims)\n",
    "    \n",
    "    \n",
    "class Relu(Activation):\n",
    "    \"\"\" all the functions related to the relu activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the relu function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return (Z > 0) * Z\n",
    "    \n",
    "    # to calculate the derivative of the relu function for the backward pass\n",
    "    def derivative(self, Z):\n",
    "        return (Z > 0) * 1\n",
    "    \n",
    "    # to be used to finetune the initialized weight if relu function is set for the first layer\n",
    "    def heuristic(self, layer_b4):\n",
    "        return np.sqrt(2 / layer_b4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    \"\"\" the abstract class for all the cost functions \"\"\"\n",
    "    \n",
    "    # calculate the cost function\n",
    "    def formula(self, A, Y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # calculate the derivative of the cost function (dA[L]) for the last layer\n",
    "    def derivative(self, A, Y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LossEntropy(Cost):\n",
    "    \"\"\" Use Loss Entropy to calculate the cost \"\"\"\n",
    "    \n",
    "    # calculate the Lose Entropy cost\n",
    "    def formula(self, A, Y):\n",
    "        self.m = Y.shape[1]\n",
    "        print(self.m)\n",
    "        print(f'Y: {Y}')\n",
    "        print(f'A: {A}')\n",
    "        return - np.sum((Y * np.log(A) + (1-Y) * np.log(1-A)), axis=1) / self.m\n",
    "    \n",
    "    # calculate the derivative of the Lost Entropy cost\n",
    "    def derivative(self, A, Y):\n",
    "        return - ((np.divide(Y, A)) - (np.divide(1-Y, 1-A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\" the abstract class for all layer classes \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None \n",
    "    \n",
    "    # implement forward pass\n",
    "    def forward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # implement backward pass\n",
    "    def backward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class FCLayer(Layer):\n",
    "    \n",
    "    # initialize parameters\n",
    "    def __init__(self, layer_b4, layer_after, activation):\n",
    "\n",
    "        self.W = np.random.randn(layer_after, layer_b4) * getattr(activation, 'heuristic')(self, layer_b4)\n",
    "        self.b = np.zeros((layer_after, 1))\n",
    "        print(f'initialized W: {self.W}')\n",
    "        print(f'initialized b: {self.b}')\n",
    "    \n",
    "    # calculate forward pass: linear fn (Z = WX + b) and non-linear (A = g(Z))\n",
    "    def forward_pass(self, X):#, activation):\n",
    "        self.A_prev = X\n",
    "        self.Z = np.dot(self.W, X) + self.b\n",
    "        self.A = getattr(activation, 'formula')(self, self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    # calculate backward pass: \n",
    "    # dZ = dA * g'(Z))\n",
    "    # dA[l-1] = W.T * dZ\n",
    "    def backward_pass(self, dA, learning_rate):    \n",
    "        print(f'shape of W: {np.shape(self.W)}')\n",
    "        self.dZ = dA * getattr(activation, 'derivative')(self, self.A)\n",
    "        pre = np.dot(self.W.T, self.dZ)\n",
    "        \n",
    "        print(f'shape of dZ: {np.shape(self.dZ)}')\n",
    "        self.dW = np.dot(self.dZ, self.A_prev.T)\n",
    "        print(f'shape of dW: {np.shape(self.dW)}')\n",
    "        \n",
    "        \n",
    "        self.W -= learning_rate * self.dW\n",
    "        self.b -= learning_rate * np.sum(self.dZ)\n",
    "        print(f'updated W: {self.W}')\n",
    "        print(f'updated b: {self.b}')\n",
    "        \n",
    "        return np.dot(self.W.T, self.dZ) # dA[l-1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\" build the whole L-layer DNN \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    # combine individual layer to form the whole DNN network\n",
    "    def combine(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # train the DNN network model\n",
    "    def fit(self, X, iteration, loss_fn):\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            \n",
    "            A = X\n",
    "            for layer in self.layers:\n",
    "                A = layer.forward_pass(A)\n",
    "                print(f'A: {A}')\n",
    "                \n",
    "            cost = getattr(loss_fn, 'formula')(A, Y)\n",
    "            print(f'cost: {cost}')\n",
    "            \n",
    "            dA = getattr(loss_fn, 'derivative')(A, Y)\n",
    "            \n",
    "            for layer in reversed(self.layers):\n",
    "                dA = layer.backward_pass(dA, learning_rate)\n",
    "                print(f'dA: {dA}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized W: [[ 0.33045622  1.56135363]\n",
      " [ 0.00253582 -0.49874634]\n",
      " [ 2.14722607 -0.056105  ]]\n",
      "initialized b: [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "initialized W: [[ 0.62100739 -0.57822457  0.36866233]]\n",
      "initialized b: [[0.]]\n",
      "A: [[0.86896175 0.8265475  0.5        0.5       ]\n",
      " [0.37843162 0.37783533 0.5        0.5       ]\n",
      " [0.89003719 0.48597743 0.5        0.5       ]]\n",
      "A: [[0.65677074 0.61632534 0.55125002 0.55125002]]\n",
      "4\n",
      "Y: [[0 1 0 0]]\n",
      "A: [[0.65677074 0.61632534 0.55125002 0.55125002]]\n",
      "cost: [0.78897894]\n",
      "shape of W: (1, 3)\n",
      "shape of dZ: (1, 4)\n",
      "shape of dW: (1, 3)\n",
      "updated W: [[ 0.5429284  -0.64074293  0.27662113]]\n",
      "updated b: [[-0.13193975]]\n",
      "dA: [[ 0.35569975 -0.20057038  0.28060449  0.28060449]\n",
      " [-0.41978298  0.23670534 -0.33115848 -0.33115848]\n",
      " [ 0.18122844 -0.10219028  0.14296753  0.14296753]]\n",
      "shape of W: (3, 2)\n",
      "shape of dZ: (3, 4)\n",
      "shape of dW: (3, 2)\n",
      "updated W: [[ 0.3230517   1.55819581]\n",
      " [ 0.01266345 -0.49433007]\n",
      " [ 2.14348621 -0.0574352 ]]\n",
      "updated b: [[-0.00441525]\n",
      " [-0.00441525]\n",
      " [-0.00441525]]\n",
      "dA: [[ 0.10280131 -0.06464651  0.09233415  0.09233415]\n",
      " [ 0.16329281 -0.09302085  0.13929304  0.13929304]]\n"
     ]
    }
   ],
   "source": [
    "# define variables\n",
    "X_size = 4\n",
    "learning_rate = 0.1\n",
    "loss_fn = LossEntropy()\n",
    "iteration = 1\n",
    "\n",
    "# generate training dataset\n",
    "ds = CreateDataset()\n",
    "X = ds.create_X(X_size)\n",
    "Y = ds.create_Y(X)\n",
    "\n",
    "# define each layer and combine them to build the whole DNN network\n",
    "net = Network()\n",
    "net.combine(FCLayer(2, 3, Relu))\n",
    "net.combine(FCLayer(3, 1, Sigmoid))\n",
    "\n",
    "# train the DNN network model\n",
    "net.fit(X, iteration, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Y: [[0 1 0 0]]\n",
      "A: [[0.75906421 0.58439413 0.58439413 0.58439413]]\n",
      "cost: [0.92911006]\n",
      "dA: [[ 4.15048342 -1.71117393  2.40612578  2.40612578]]\n",
      "[[0.67487298 0.03095926]]\n",
      "[[0.01965673]]\n",
      "[[ 0.53819613 -0.22188914  0.40078088  0.40078088]\n",
      " [ 0.02468932 -0.01017899  0.0183855   0.0183855 ]]\n"
     ]
    }
   ],
   "source": [
    "cost = loss_fn.formula(A, Y)\n",
    "print(f'cost: {cost}')\n",
    "\n",
    "dA = loss_fn.derivative(A, Y)\n",
    "print(f'dA: {dA}')\n",
    "dA = layer.backward_pass(dA, learning_rate)\n",
    "print(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9525741268224334\n",
      "-2\n",
      "0.7071067811865476\n",
      "0.9950547536867306\n",
      "-3\n",
      "0.7071067811865476\n",
      "3\n",
      "1\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "actS = Sigmoid()\n",
    "print(actS.formula(3))\n",
    "print(actS.derivative(2))\n",
    "print(actS.heuristic(2))\n",
    "\n",
    "actT = Tanh()\n",
    "print(actT.formula(3))\n",
    "print(actT.derivative(2))\n",
    "print(actT.heuristic(2))\n",
    "\n",
    "actR = Relu()\n",
    "print(actR.formula(3))\n",
    "print(actR.derivative(2))\n",
    "print(actR.heuristic(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50833031, 0.84784872, 0.2268602 ],\n",
       "       [0.50219847, 0.05825357, 0.37045537],\n",
       "       [0.98577938, 0.00154966, 0.89227618],\n",
       "       [0.22882473, 0.01854588, 0.51847913]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(6).reshape((3, 2))\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = a.reshape((2, 3))\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [1 1 0 0]]\n",
      "[[0 1 0 0]]\n",
      "(1, 4)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ds = CreateDataset()\n",
    "X_size = 4\n",
    "X = ds.create_X(X_size)\n",
    "print(X)\n",
    "Y = ds.create_Y(X)\n",
    "print(Y)\n",
    "print(np.shape(Y))\n",
    "print(Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0]\n",
      " [0 1 1 0]]\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 1 0]\n",
      "[ True False False  True]\n",
      "[0 1 1 0]\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "X_size = 4\n",
    "X = np.random.randint(2, size=(2,X_size))\n",
    "print(X)\n",
    "print(type(X))\n",
    "Y = np.sum(X, axis=0)\n",
    "print(Y)\n",
    "print((Y != 1))\n",
    "Y[Y != 1] = 0\n",
    "print(Y)\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
