{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset:\n",
    "    \"\"\" create the 2 XOR datasets, X & Y, for the DNN model \"\"\"\n",
    "    \n",
    "    # create the entry dataset X\n",
    "    def create_X(self, X_size):\n",
    "        self.X = np.random.randint(2, size=(2, X_size))\n",
    "        return self.X\n",
    "    \n",
    "    # create the label dataset Y\n",
    "    def create_Y(self, X):\n",
    "        self.Y = np.sum(X, axis=0)\n",
    "        self.Y[self.Y != 1] = 0\n",
    "        return self.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\" the abstract class for all activation funtion classes\"\"\"\n",
    "    \n",
    "    # the basic formula of the activation function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "    # to calculate the derivative of the activation function for the backward pass\n",
    "    def derivative(self, input):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "    # to be used to finetune the initialized weight according to the activation function set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\" all the functions related to the sigmoid activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the sigmoid function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # to calculate the derivative of the sigmoid function for the backward pass\n",
    "    def derivative(self, Z):\n",
    "        self.A = Sigmoid.formula(self, Z)\n",
    "        return self.A * (1 - self.A)\n",
    "    \n",
    "    # to be used to finetune the initialized weight if sigmoid function is set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        return np.sqrt(1 / layer_dims)\n",
    "    \n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\" all the functions related to the tanh activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the tanh function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    \n",
    "    # to calculate the derivative of the tanh function for the backward pass\n",
    "    def derivative(self, Z):\n",
    "        self.A = Tanh.formula(self, Z)\n",
    "        return 1 - self.A**2\n",
    "    \n",
    "    # to be used to finetune the initialized weight if tanh function is set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        return np.sqrt(1 / layer_dims)\n",
    "    \n",
    "    \n",
    "class Relu(Activation):\n",
    "    \"\"\" all the functions related to the relu activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the relu function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return (Z > 0) * Z\n",
    "    \n",
    "    # to calculate the derivative of the relu function for the backward pass\n",
    "    def derivative(self, Z):\n",
    "        return (Z > 0) * 1\n",
    "    \n",
    "    # to be used to finetune the initialized weight if relu function is set for the first layer\n",
    "    def heuristic(self, layer_b4):\n",
    "        return np.sqrt(2 / layer_b4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\" the abstract class for all layer classes \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    # implement forward pass\n",
    "    def forward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # implement backward pass\n",
    "    def backward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class FCLayer(Layer):\n",
    "    \n",
    "    # initialize parameters\n",
    "    def __init__(self, layer_b4, layer_after, activation):\n",
    "\n",
    "        self.W = np.random.randn(layer_after, layer_b4) * getattr(activation, 'heuristic')(self, layer_b4)\n",
    "        self.b = np.zeros((layer_after, 1))\n",
    "        print(self.W)\n",
    "        print(self.b)\n",
    "    \n",
    "    # calculate forward pass: linear fn (Z = WX + b) and non-linear (A = g(Z))\n",
    "    def forward_pass(self, X, activation):\n",
    "        \n",
    "        self.Z = np.dot(self.W, X) + self.b\n",
    "        self.A = getattr(activation, 'formula')(self, self.Z)\n",
    "        return self.Z, self.A\n",
    "    \n",
    "    # calculate backward pass: \n",
    "    # dA[L] = -(Y/A) + ((1-Y)/(1-A))\n",
    "    # dZ = dA * g'(Z))\n",
    "    # dA[l-1] = W.T * dZ\n",
    "    def backward_pass(self, dA):    \n",
    "        self.dZ = dA * getattr(activation, 'derivative')(self, self.Z)\n",
    "        return np.dot(self.W.T, self.dZ) # dA[l-1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54491188 0.49700392]\n",
      " [0.36887662 1.25592451]]\n",
      "[[0.]\n",
      " [0.]]\n",
      "[[0.54491188 0.49700392 1.0419158  0.54491188]\n",
      " [0.36887662 1.25592451 1.62480114 0.36887662]] [[0.63295431 0.62175498 0.73921949 0.63295431]\n",
      " [0.5911875  0.77832374 0.8354562  0.5911875 ]]\n",
      "[[ 0.01169974  0.01389171 -0.00368094  0.01169974]\n",
      " [ 0.02182446  0.02317318 -0.00613412  0.02182446]]\n"
     ]
    }
   ],
   "source": [
    "activation = Sigmoid\n",
    "layer = FCLayer(2, 2, activation)\n",
    "Z, A = layer.forward_pass(X, activation)\n",
    "print(Z, A)\n",
    "# dA = layer.backward_pass(Y=Y)\n",
    "dA = layer.backward_pass(dA)\n",
    "print(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1]\n",
      " [0 1 1 0]]\n",
      "[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "ds = CreateDataset()\n",
    "X_size = 4\n",
    "X = ds.create_X(X_size)\n",
    "print(X)\n",
    "Y = ds.create_Y(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0]\n",
      " [1 0 1 1]]\n",
      "<class 'numpy.ndarray'>\n",
      "[1 1 2 1]\n",
      "[False False  True False]\n",
      "[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "X_size = 4\n",
    "X = np.random.randint(2, size=(2,X_size))\n",
    "print(X)\n",
    "print(type(X))\n",
    "Y = np.sum(X, axis=0)\n",
    "print(Y)\n",
    "print((Y != 1))\n",
    "Y[Y != 1] = 0\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9525741268224334\n",
      "0.10499358540350662\n",
      "0.7071067811865476\n",
      "0.9950547536867306\n",
      "0.0706508248531642\n",
      "0.7071067811865476\n",
      "3\n",
      "1\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "actS = Sigmoid()\n",
    "print(actS.formula(3))\n",
    "print(actS.derivative(2))\n",
    "print(actS.heuristic(2))\n",
    "\n",
    "actT = Tanh()\n",
    "print(actT.formula(3))\n",
    "print(actT.derivative(2))\n",
    "print(actT.heuristic(2))\n",
    "\n",
    "actR = Relu()\n",
    "print(actR.formula(3))\n",
    "print(actR.derivative(2))\n",
    "print(actR.heuristic(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87558993, 0.93684569, 0.94847332],\n",
       "       [0.96739116, 0.23928585, 0.85061525],\n",
       "       [0.06319912, 0.46525894, 0.21063519],\n",
       "       [0.9365597 , 0.49253958, 0.06764355]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.dA = - (np.divide(Y, self.A)) + (np.divide(1-Y, 1-self.A))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
