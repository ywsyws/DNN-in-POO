{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset:\n",
    "    \"\"\" create the 2 XOR datasets, X & Y, for the DNN model \"\"\"\n",
    "    \n",
    "    # create the entry dataset X\n",
    "    def create_X(self, X_size):\n",
    "        self.X = np.random.randint(2, size=(2, X_size))\n",
    "        return self.X\n",
    "    \n",
    "    # create the label dataset Y\n",
    "    def create_Y(self, X):\n",
    "        self.Y = np.sum(X, axis=0)\n",
    "        self.Y[self.Y != 1] = 0\n",
    "        return self.Y.reshape((1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\" the abstract class for all activation funtion classes\"\"\"\n",
    "    \n",
    "    # the basic formula of the activation function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "    # to calculate the derivative of the activation function for the backward pass\n",
    "    def derivative(self, input):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "    # to be used to finetune the initialized weight according to the activation function set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\" all the functions related to the sigmoid activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the sigmoid function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # to calculate the derivative of the sigmoid function for the backward pass\n",
    "    def derivative(self, Z):\n",
    "        self.A = Sigmoid.formula(self, Z)\n",
    "        return self.A * (1 - self.A)\n",
    "    \n",
    "    # to be used to finetune the initialized weight if sigmoid function is set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        return np.sqrt(1 / layer_dims)\n",
    "    \n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\" all the functions related to the tanh activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the tanh function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    \n",
    "    # to calculate the derivative of the tanh function for the backward pass\n",
    "    def derivative(self, Z):\n",
    "        self.A = Tanh.formula(self, Z)\n",
    "        return 1 - self.A**2\n",
    "    \n",
    "    # to be used to finetune the initialized weight if tanh function is set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        return np.sqrt(1 / layer_dims)\n",
    "    \n",
    "    \n",
    "class Relu(Activation):\n",
    "    \"\"\" all the functions related to the relu activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the relu function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return (Z > 0) * Z\n",
    "    \n",
    "    # to calculate the derivative of the relu function for the backward pass\n",
    "    def derivative(self, Z):\n",
    "        return (Z > 0) * 1\n",
    "    \n",
    "    # to be used to finetune the initialized weight if relu function is set for the first layer\n",
    "    def heuristic(self, layer_b4):\n",
    "        return np.sqrt(2 / layer_b4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\" the abstract class for all layer classes \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    # implement forward pass\n",
    "    def forward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # implement backward pass\n",
    "    def backward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class FCLayer(Layer):\n",
    "    \n",
    "    # initialize parameters\n",
    "    def __init__(self, layer_b4, layer_after, activation):\n",
    "\n",
    "        self.W = np.random.randn(layer_after, layer_b4) * getattr(activation, 'heuristic')(self, layer_b4)\n",
    "        self.b = np.zeros((layer_after, 1))\n",
    "        print(self.W)\n",
    "        print(self.b)\n",
    "    \n",
    "    # calculate forward pass: linear fn (Z = WX + b) and non-linear (A = g(Z))\n",
    "    def forward_pass(self, X):#, activation):\n",
    "        \n",
    "        self.Z = np.dot(self.W, X) + self.b\n",
    "        self.A = getattr(activation, 'formula')(self, self.Z)\n",
    "        return self.Z, self.A\n",
    "    \n",
    "    # calculate backward pass: \n",
    "    # dA[L] = -(Y/A) + ((1-Y)/(1-A))\n",
    "    # dZ = dA * g'(Z))\n",
    "    # dA[l-1] = W.T * dZ\n",
    "    def backward_pass(self, dA, learning_rate):    \n",
    "        self.dZ = dA * getattr(activation, 'derivative')(self, self.Z)\n",
    "        self.dW = np.dot(self.dZ, self.A.T)\n",
    "        \n",
    "        self.W -= learning_rate * self.dW\n",
    "        self.b -= learning_rate * np.sum(self.dZ)\n",
    "        print(self.W)\n",
    "        print(self.b)\n",
    "        \n",
    "        return np.dot(self.W.T, self.dZ) # dA[l-1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    \"\"\" the abstract class for all the cost functions \"\"\"\n",
    "    \n",
    "    # calculate the cost function\n",
    "    def formula(self, A, Y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # calculate the derivative of the cost function (dA[L]) for the last layer\n",
    "    def derivative(self, A, Y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LossEntropy(Cost):\n",
    "    \"\"\" Use Loss Entropy to calculate the cost \"\"\"\n",
    "    \n",
    "    # calculate the Lose Entropy cost\n",
    "    def formula(self, A, Y):\n",
    "        self.m = Y.shape[1]\n",
    "        print(self.m)\n",
    "        print(f'Y: {Y}')\n",
    "        print(f'A: {A}')\n",
    "        return - np.sum((Y * np.log(A) + (1-Y) * np.log(1-A)), axis=1) / self.m\n",
    "    \n",
    "    # calculate the derivative of the Lost Entropy cost\n",
    "    def derivative(self, A, Y):\n",
    "        return - (np.divide(Y, A)) + (np.divide(1-Y, 1-A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70970015 0.06578643]]\n",
      "[[0.]]\n",
      "[[0.06578643 0.06578643 0.70970015 0.06578643]] [[0.51644068 0.51644068 0.6703349  0.51644068]]\n",
      "4\n",
      "Y: [[1 1 1 1]]\n",
      "A: [[0.51644068 0.51644068 0.6703349  0.51644068]]\n",
      "cost: [0.5955906]\n",
      "dA: [[-1.93633082 -1.93633082 -1.49179164 -1.93633082]]\n",
      "[[0.80671766 0.16280394]]\n",
      "[[0.17803431]]\n",
      "[[-0.39009584 -0.39009584 -0.26594666 -0.39009584]\n",
      " [-0.07872536 -0.07872536 -0.05367078 -0.07872536]]\n"
     ]
    }
   ],
   "source": [
    "activation = Sigmoid\n",
    "learning_rate = 0.1\n",
    "loss_fn = LossEntropy()\n",
    "\n",
    "layer = FCLayer(2, 1, activation)\n",
    "\n",
    "Z, A = layer.forward_pass(X)\n",
    "print(Z, A)\n",
    "\n",
    "cost = loss_fn.formula(A, Y)\n",
    "print(f'cost: {cost}')\n",
    "\n",
    "dA = loss_fn.derivative(A, Y)\n",
    "print(f'dA: {dA}')\n",
    "dA = layer.backward_pass(dA, learning_rate)\n",
    "print(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9525741268224334\n",
      "0.10499358540350662\n",
      "0.7071067811865476\n",
      "0.9950547536867306\n",
      "0.0706508248531642\n",
      "0.7071067811865476\n",
      "3\n",
      "1\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "actS = Sigmoid()\n",
    "print(actS.formula(3))\n",
    "print(actS.derivative(2))\n",
    "print(actS.heuristic(2))\n",
    "\n",
    "actT = Tanh()\n",
    "print(actT.formula(3))\n",
    "print(actT.derivative(2))\n",
    "print(actT.heuristic(2))\n",
    "\n",
    "actR = Relu()\n",
    "print(actR.formula(3))\n",
    "print(actR.derivative(2))\n",
    "print(actR.heuristic(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87558993, 0.93684569, 0.94847332],\n",
       "       [0.96739116, 0.23928585, 0.85061525],\n",
       "       [0.06319912, 0.46525894, 0.21063519],\n",
       "       [0.9365597 , 0.49253958, 0.06764355]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(6).reshape((3, 2))\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = a.reshape((2, 3))\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]\n",
      " [1 1 0 1]]\n",
      "[[1 1 1 1]]\n",
      "(1, 4)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ds = CreateDataset()\n",
    "X_size = 4\n",
    "X = ds.create_X(X_size)\n",
    "print(X)\n",
    "Y = ds.create_Y(X)\n",
    "print(Y)\n",
    "print(np.shape(Y))\n",
    "print(Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [0 0 1 1]]\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 2 2]\n",
      "[ True False  True  True]\n",
      "[0 1 0 0]\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "X_size = 4\n",
    "X = np.random.randint(2, size=(2,X_size))\n",
    "print(X)\n",
    "print(type(X))\n",
    "Y = np.sum(X, axis=0)\n",
    "print(Y)\n",
    "print((Y != 1))\n",
    "Y[Y != 1] = 0\n",
    "print(Y)\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
