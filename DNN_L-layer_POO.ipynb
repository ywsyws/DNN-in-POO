{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset:\n",
    "    \"\"\" create the 2 XOR datasets, X & Y, for the DNN model \"\"\"\n",
    "    \n",
    "    # create the entry dataset X\n",
    "    def create_X(self, X_size):\n",
    "        self.X = np.random.randint(2, size=(2, X_size))\n",
    "        return self.X\n",
    "    \n",
    "    # create the label dataset Y\n",
    "    def create_Y(self, X):\n",
    "        self.Y = np.sum(X, axis=0).reshape((1,4))\n",
    "        self.Y[self.Y != 1] = 0\n",
    "        return self.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\" the abstract class for all activation funtion classes\"\"\"\n",
    "    \n",
    "    # the basic formula of the activation function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "    # to calculate the derivative of the activation function for the backward pass\n",
    "    def derivative(self, input):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "    # to be used to finetune the initialized weight according to the activation function set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        raise NotImplementdError\n",
    "    \n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\" all the functions related to the sigmoid activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the sigmoid function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # to calculate the derivative of the sigmoid function for the backward pass\n",
    "    def derivative(self, A):\n",
    "        return A * (1 - A)\n",
    "    \n",
    "    # to be used to finetune the initialized weight if sigmoid function is set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        return np.sqrt(1 / layer_dims)\n",
    "    \n",
    "\n",
    "class Tanh(Activation):\n",
    "    \"\"\" all the functions related to the tanh activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the tanh function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    \n",
    "    # to calculate the derivative of the tanh function for the backward pass\n",
    "    def derivative(self, A):\n",
    "        return 1 - A**2\n",
    "    \n",
    "    # to be used to finetune the initialized weight if tanh function is set for the first layer\n",
    "    def heuristic(self, layer_dims):\n",
    "        return np.sqrt(1 / layer_dims)\n",
    "    \n",
    "    \n",
    "class Relu(Activation):\n",
    "    \"\"\" all the functions related to the relu activation function \"\"\"\n",
    "    \n",
    "    # the basic formula of the relu function for the forward pass\n",
    "    def formula(self, Z):\n",
    "        return (Z > 0) * Z\n",
    "    \n",
    "    # to calculate the derivative of the relu function for the backward pass\n",
    "    def derivative(self, Z):\n",
    "        return (Z > 0) * 1\n",
    "    \n",
    "    # to be used to finetune the initialized weight if relu function is set for the first layer\n",
    "    def heuristic(self, layer_b4):\n",
    "        return np.sqrt(2 / layer_b4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    \"\"\" the abstract class for all the cost functions \"\"\"\n",
    "    \n",
    "    # calculate the cost function\n",
    "    def formula(self, A, Y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # calculate the derivative of the cost function (dA[L]) for the last layer\n",
    "    def derivative(self, A, Y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LossEntropy(Cost):\n",
    "    \"\"\" Use Loss Entropy to calculate the cost \"\"\"\n",
    "    \n",
    "    # calculate the Lose Entropy cost\n",
    "    def formula(self, A, Y):\n",
    "        self.m = Y.shape[1]\n",
    "#         print(self.m)\n",
    "#         print(f'Y: {Y}')\n",
    "#         print(f'A: {A}')\n",
    "        return - np.sum((Y * np.log(A) + (1-Y) * np.log(1-A)), axis=1) / self.m\n",
    "    \n",
    "    # calculate the derivative of the Lost Entropy cost\n",
    "    def derivative(self, A, Y):\n",
    "        return - ((np.divide(Y, A)) - (np.divide(1-Y, 1-A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\" the abstract class for all layer classes \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None \n",
    "    \n",
    "    # implement forward pass\n",
    "    def forward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # implement backward pass\n",
    "    def backward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class FCLayer(Layer):\n",
    "    \n",
    "    # initialize parameters\n",
    "    def __init__(self, layer_b4, layer_after, activation):\n",
    "\n",
    "        self.activation = activation\n",
    "        self.W = np.random.randn(layer_after, layer_b4) * getattr(self.activation, 'heuristic')(self, layer_b4)\n",
    "        self.b = np.zeros((layer_after, 1))\n",
    "#         print(f'initialized W: {self.W}')\n",
    "#         print(f'initialized b: {self.b}')\n",
    "    \n",
    "    # calculate forward pass: linear fn (Z = WX + b) and non-linear (A = g(Z))\n",
    "    def forward_pass(self, X):#, activation):\n",
    "        self.A_prev = X\n",
    "        self.Z = np.dot(self.W, X) + self.b\n",
    "        self.A = getattr(self.activation, 'formula')(self, self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    # calculate backward pass: \n",
    "    # dZ = dA * g'(Z))\n",
    "    # dA[l-1] = W.T * dZ\n",
    "    def backward_pass(self, dA, learning_rate):    \n",
    "#         print(f'shape of W: {np.shape(self.W)}')\n",
    "        self.dZ = dA * getattr(self.activation, 'derivative')(self, self.A)\n",
    "        pre = np.dot(self.W.T, self.dZ)\n",
    "        \n",
    "#         print(f'shape of dZ: {np.shape(self.dZ)}')\n",
    "        self.dW = np.dot(self.dZ, self.A_prev.T)\n",
    "#         print(f'shape of dW: {np.shape(self.dW)}')\n",
    "        \n",
    "        \n",
    "        self.W -= learning_rate * self.dW\n",
    "        self.b -= learning_rate * np.sum(self.dZ)\n",
    "#         print(f'updated W: {self.W}')\n",
    "#         print(f'updated b: {self.b}')\n",
    "        \n",
    "        return np.dot(self.W.T, self.dZ) # dA[l-1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\" build the whole L-layer DNN \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    # combine individual layer to form the whole DNN network\n",
    "    def combine(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    # train the DNN model\n",
    "    def fit(self, X, iteration, loss_fn, learning_rate):\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            \n",
    "            A = X\n",
    "            for layer in self.layers:\n",
    "                A = layer.forward_pass(A)\n",
    "#                 print(f'A: {A}')\n",
    "                \n",
    "            cost = getattr(loss_fn, 'formula')(A, Y)\n",
    "            if i % 10 == 0:\n",
    "                print(f'cost de {i}: {cost}')\n",
    "#             print(f'cost de {i}: {cost}')\n",
    "            \n",
    "            dA = getattr(loss_fn, 'derivative')(A, Y)\n",
    "            \n",
    "            for layer in reversed(self.layers):\n",
    "                dA = layer.backward_pass(dA, learning_rate)\n",
    "#                 print(f'dA: {dA}')\n",
    "\n",
    "    # predict test dataset using the trained DNN model\n",
    "    def asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost de 0: [0.72884888]\n",
      "cost de 10: [0.60998626]\n",
      "cost de 20: [0.54405236]\n",
      "cost de 30: [0.48705801]\n",
      "cost de 40: [0.4309579]\n",
      "cost de 50: [0.37164736]\n",
      "cost de 60: [0.30684325]\n",
      "cost de 70: [0.23914716]\n",
      "cost de 80: [0.17710132]\n",
      "cost de 90: [0.13253443]\n"
     ]
    }
   ],
   "source": [
    "# define variables\n",
    "X_size = 4\n",
    "learning_rate = 0.1\n",
    "loss_fn = LossEntropy()\n",
    "iteration = 100\n",
    "\n",
    "# generate training dataset\n",
    "ds = CreateDataset()\n",
    "X = ds.create_X(X_size)\n",
    "Y = ds.create_Y(X)\n",
    "\n",
    "# define each layer and combine them to build the whole DNN network\n",
    "net = Network()\n",
    "net.combine(FCLayer(2, 3, Relu))\n",
    "net.combine(FCLayer(3, 1, Sigmoid))\n",
    "\n",
    "# train the DNN network model\n",
    "net.fit(X, iteration, loss_fn, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-88bd83278118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'cost: {cost}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'dA: {dA}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "cost = loss_fn.formula(A, Y)\n",
    "print(f'cost: {cost}')\n",
    "\n",
    "dA = loss_fn.derivative(A, Y)\n",
    "print(f'dA: {dA}')\n",
    "dA = layer.backward_pass(dA, learning_rate)\n",
    "print(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actS = Sigmoid()\n",
    "print(actS.formula(3))\n",
    "print(actS.derivative(2))\n",
    "print(actS.heuristic(2))\n",
    "\n",
    "actT = Tanh()\n",
    "print(actT.formula(3))\n",
    "print(actT.derivative(2))\n",
    "print(actT.heuristic(2))\n",
    "\n",
    "actR = Relu()\n",
    "print(actR.formula(3))\n",
    "print(actR.derivative(2))\n",
    "print(actR.heuristic(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(6).reshape((3, 2))\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.reshape((2, 3))\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = CreateDataset()\n",
    "X_size = 4\n",
    "X = ds.create_X(X_size)\n",
    "print(X)\n",
    "Y = ds.create_Y(X)\n",
    "print(Y)\n",
    "print(np.shape(Y))\n",
    "print(Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_size = 4\n",
    "X = np.random.randint(2, size=(2,X_size))\n",
    "print(X)\n",
    "print(type(X))\n",
    "Y = np.sum(X, axis=0)\n",
    "print(Y)\n",
    "print((Y != 1))\n",
    "Y[Y != 1] = 0\n",
    "print(Y)\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
